# [译: Kafka Design](https://kafka.apache.org/documentation/#design)

## 动机

Kafka 被设计为一个统一的平台用于处理所有类型的实时数据流，所以我们需要考虑非常多的使用场景。

它需要有较大的吞吐量来支撑像实时日志聚合这样大规模的事件流。

它需要处理庞大的数据积压来支持离线系统定期加载数据。

它同时也需要像传统的消息系统一样支持低延迟的数据发送。

我们希望提供分区的、分布式的、实时的信息流处理方式，这也是我们打造 Kafka 分区和消费模型的灵感源泉。

Kafka 还需要支持将数据流喂给其他数据系统的情况，因此需要有机器故障时的容错能力。

为了支撑上述的使用场景，我们设计了许多独立的元件，与传统的消息系统相比，更像是数据库日志系统。我们将在接下来的几节中概述其中一些元件的设计。

## 持久化

### 不要畏惧文件系统！

Kafka 非常依赖文件系统来存储和缓存消息。人们通常会有一种磁盘很慢的感觉，这使得他们通常会怀疑持久性存储的性能。事实上，磁盘可以很慢也可以很快，这完全取决于如何使用磁盘；一个设计得当的磁盘系统可以网络一样快。

一个非常关键的事实是，在过去的十年里，磁盘驱动的吞吐量和磁盘的寻道延时不匹配。这就导致在一个拥有六个7200rpm SATA RAID-5 阵列的 JBOD 上线性写的性能是 600 MB/秒，而随机写只有 100 KB/秒，相差6000 倍。线性读写在许多使用场景中是可预测的，而且操作系统对此有很多的优化。现代操作系统提供的 Read ahead 和 Write behind 技术可以对数据做大块的预先读取以及把分散的小的逻辑写合并成一个大的物理写入。关于这个问题的讨论，详见[ACM论文](https://queue.acm.org/detail.cfm?id=1563874)，他们甚至发现[在某些情况下，顺序磁盘读取比随机内存访问还快](https://deliveryimages.acm.org/10.1145/1570000/1563874/jacobs3.jpg)

为了弥补磁盘驱动与磁盘寻道性能上的差异，现代操作系统越来越倾向于使用内存做磁盘的缓存。在进行内存回收时，现代系统倾向于使用所有可用内存做磁盘缓存。所有的磁盘读写都统一使用这一缓存系统。除非使用 Direct I/O，否则即使程序已经维护了数据在进程内的高速缓存，操作系统也会在页缓存中再存一遍这些数据。

而且，我们的系统建立在 JVM 上，稍微了解过 Java 内存使用方式的人都应该知道：

1. 对象存储的开销非常高，通常是实际数据大小的一倍（甚至更严重）。
2. 随着对堆内数据的增加，Java 的垃圾回收会越来越慢。

基于上述原因，使用文件系统和页缓存要优于维护内存 cache 和其他结构，至少我们节省出了一倍的缓存，更进一步，使用紧凑的字节结构而非单个对象的存储，可以将所使用的存储空间再节省一半。通过上述方式，我们可以在 32 GB 的计算机上缓存 28-30 GB 的数据，而且没有 GC 的困扰。而且当进程重启时，缓存仍然可用，而如果使用进程内缓存的话，需要在内存中重建数据缓存（10 GB 的缓存可能需要10分钟），或者选择冷启动（初始性能会非常差）。这样做还有一个好处就是代码简洁，由操作系统来处理文件系统和缓存之间的逻辑。如果你的磁盘支持线性读取，那么每次读磁盘时执行的预读会在缓存中填充邻近的有效数据。

这样的设计非常简单，而且我们反对在进程内维护尽可能多的内存，然后在没有可用空间的时候再同步到文件系统的操作。所有的数据直接写入到文件系统而不用刷新到磁盘，实际上只是被放到了内核的页缓存里。

这种以页缓存为中心的设计风格在[这篇文章](http://varnish-cache.org/docs/trunk/phk/notes.html)中进行了描述。

### 常数时间就够了

通常，消息系统里的持久化数据结构都是一个消费者一个队列以及与其关联的 BTree 或其他通用的随机访问数据结构，用于维护消息的元数据。BTree 是最通用的数据结构，可以在消息系统中实现各种事务与非事务语义。但通常也伴随着大量的性能损耗，操作 BTree 时间复杂度是 O(logN)。通常 O(logN) 被认为与常数复杂度相差无几，这在磁盘操作中却不是这样。磁盘的一次寻道是要 10 毫秒，而且每个磁盘同一时间只能执行一次寻道，而且即使是少量的磁盘寻道也会导致很高的性能开销。由于存储系统将非常快的缓存操作和非常慢的磁盘操作混合在一起，当数据不断增加时，树结构的性能损耗通常是超过线性增长的。

直观上讲，一个持久性的队列可以通过对文件的简单读取和追加操作实现，通常和日志记录的解决方案是一样的。这个结构有一个好处就是所有的操作都是O(1)的，而且读操作并不会阻塞写以及其他操作。这样的结构具有明显的性能优势，性能与数据量大小完全脱钩，服务器可以充分利用廉价的1TB 磁盘。尽管这些磁盘的寻道性能较差，但是在大规模数据的读写上性能是可接受的，且价格低廉。

可以访问几乎无限的磁盘空间而不会降低性能，这意味着我们可以提供传统消息系统不具备的功能。例如，Kafka 可以保留已消费的消息很长一段时间，而不用像传统消息系统那样直接删除，这就为消费者提供了非常棒的灵活性。

## 效率

为提高 Kafka 的效率，我们付出了很多努力。一个主要的场景就是处理活跃的网络数据，这个数据量非常大：每个页都可能会产生数十次的写入操作。而且我们假设每一条消息都可能被多个消费者消费，因此我们需要让消费操作尽可能 cheap。

我们从构造和运行很多相似系统的经验发现，这是提高这种多租户操作效率的关键。如果下游基础服务很容易因为业务量的小幅度增加而成为瓶颈，那么这会是一个非常常见的问题。Kafka 可以帮助业务系统在有负载的情况下先于基础服务升级。当在运行支持大集群上数十或数百个业务系统中心化服务时，这是很重要的能力，因为这种使用模式上的变化几乎每天都在发生。

我们在上一节中讨论了磁盘的效率。一旦消除了不良的磁盘访问模式，系统效率低下的常见原因一般有两个：过多的小的I/O操作和过多的字节拷贝。

小 I/O 问题在客户端和服务器之间以及服务器自己的持久化操作中都会发生。

为了避免这个问题，我们的通信协议围绕消息集合这一抽象构建。这样请求就可以将消息组合在一起来分摊网络开销，而不用一次次的发单个的消息。然后服务器再将消息集合一次性添加到持久化日志中，消费者也可以一次性获取大块的消息集合。

这一简单的优化可以把速度提高一个数量级。批量操作带来的大的网络报文，大的连续的磁盘操作，连续的内存快等等，所有这些都可以让 Kafka 将突发的随机消息写入流转换成线性写入流，然后流向消费者。

另外一个低效的问题就是字节拷贝，在消息量较小的时候，这还不算是一个问题，但是当负载比较大时，影响就愈发明显了。为了避免这种情况，我们采用了标准的二进制消息格式，这个格式在生产者、服务器和消费者之间共享（因此消息集合可以在它们之间无修改的传递）。

服务器使用一个目录的文件维护消息日志，每个目录里的文件都由消息集合填充，这些消息集合以生产者消费者可直接使用的格式写入磁盘。保持这种通用的格式可以优化一个非常重要的操作：持久化日志块的网络传输。现代UNIX 操作系统提供了一种非常好的方式可以直接把页缓存发送给套接字。比如 Linux 的 [sendfile](https://man7.org/linux/man-pages/man2/sendfile.2.html)系统调用。

要理解 sendfile 的能力，需要先理解通常数据从文件到 socket 的流转路径：

1. 操作系统将数据从磁盘读到内核的页缓存
2. 应用程序将数据从内核读到用户态的缓存中
3. 应用程序将数据写回到内核中的 socket 缓存中
4. 操作系统将 socket 缓存中的数据拷贝到网卡缓存中，然后发出去

这需要四次拷贝，两次系统调用，显然是效率不高的。使用 sendfile 可以直接把页缓存的数据拷贝到网卡缓存，

我们设想最常见的一种情况，某个主题上有多个消费者，这样消息只需要读取到页缓存中，然后每个消费者的读取都只需要拷贝到网卡，而不需要进入用户态内存空间。这样就可以以接近网络连接的速率消费消息了。

这种页缓存和零拷贝的组合意味着，在一个 Kafka 集群上，可能看不到磁盘读取，完全从缓存中读取数据。

想了解更多 Java 对 sendfile 和零拷贝的支持，可以参考这篇[文章](https://developer.ibm.com/articles/j-zerocopy/)

### 端到端的批量压缩

在某些场景中，瓶颈不在CPU 和磁盘而在网络带宽。对于在广域网里的数据中心之间发送消息的数据管道而言，尤为如此。当然，用户可以直接对消息进行压缩，而无需 Kafka 的支持，但是这样的压缩率不高，因为大量的冗余发生在同类型消息之间的重复（比如 JSON 中的字段名、网络日志中的 user-agents、常见的字符串）。有效的压缩需要把多个消息一起压缩，而不是压缩单个消息。

Kafka 以有效的批处理格式支持批量压缩。可以将以批消息压缩后发给服务器，这批消息将以压缩的形式写入日志，仅由消费者解压缩。

Kafka 支持 GZIP、Snappy 和 ZStandard 压缩协议，有关压缩的更多信息参见[此处](https://cwiki.apache.org/confluence/display/KAFKA/Compression)

## 生产者

### 负载均衡

生产者直接向分区 Leader 发送消息，而不需要经过中间路由层。为达到这一目的，所有的 Kafka 节点都需要知道哪些服务器存活、某个 topic 特定分区的 Leader 是谁。

由客户端控制消息发往哪个分区。这个过程可以是随机的，也可以是负载均衡的，也可以是基于某种规则的。我们暴露了一个分区规则的接口，通过让用户对消息指定一个 Key 并通过对 Key 哈希来进行分区。比如使用 user id 做 Key，那么这个用户的所有数据都会被发送到同一个分区。这样可以让消费者进消费某个特定分区，来达到只接收特定消息的目的。

### 异步发送

批处理是提高效率的主要方式之一，而要启用批处理，Kafka 生产者就需要在内存中累积消息，然后再单个请求中一起发送。批处理可以配置为累计不超过多少条消息，并且等待不超过特定时间。这样可以累积更多字节再发送，让服务器只执行频率较低的大 I/O 操作。这个缓冲是可配置的，并且提供机制来允许少量额外的延迟以提高吞吐量。

关于这个[配置](https://kafka.apache.org/documentation/#producerconfigs)的详情以及生产者的[API](http://kafka.apache.org/082/javadoc/index.html?org/apache/kafka/clients/producer/KafkaProducer.html)可以在别处获取。

## 消费者

Kafka 消费者直接向所要消费的 topic 的特定分区的 Leader 发送 fetch 请求。消费者在请求中携带偏移量，然后就会接收到服务器发来的以对应偏移量开头的日志块。消费者可以通过这个偏移量来重新消费消息。

### 推与拉

我们首先要考虑的第一个问题就是，消费者从服务器拉数据还是服务器直接向消费者推数据。在这一问题上，和大多数的消息系统一样，Kafka 使用了更加传统的设计：生产者推消息到服务器，消费者从服务器拉消息。某些以日志记录为中心的系统，遵循相反的逻辑，直接将数据推送给下游。这两种方式都各自有优缺点。但是基于推送的系统有一个难点是要与各种各样的消费者打交道，服务器要控制推送数据的速率。通常的目标是让消费者以最大的速度消费，但是，在推送系统中，当消费者的速率低于生产者的速率时，消费者往往不知所措（类似一种拒绝服务攻击）。而在拉消息模型中，消费者的速度完全可以低于生产者，并且在可能的情况下跟上。不过在推模型中，也可以使用某种退避协议来表明需要讲推送的速度降下来，但是在这种情况下，充分发挥消费者的消费能力会比较困难。之前基于推模型搭建系统的经验让我们选择了更传统的拉模型。

拉模型的另一个优点是可以更积极的对发送给消费者的消息做批处理。推模型必须要考虑是直接发送当前消息，还是在不知道下游消费者是否有足够消费能力累积消息延迟发送。如果考虑低延时，那么每一条消息都需要单独发送，这样就会造成带宽的浪费。而在拉模型下，消费者总是拉取足够的消息，从日志当前的位置到某个可配置的最大长度。这样就可以在不引入不必要的延迟下，获得最佳的批处理效果。

原始的拉模型有一个不足之处：当服务器没数据时，消费者会陷入一个频繁的轮询中，知道新数据到达。为了避免频发发请求的情况，我们提供了一些配置，可以让消费者阻塞在一个长时间的请求中，直到新消息或者给定字节数的新消息集合到达。

你可以设想别的端到端拉模型的设计。生产者把消息日志写到本地，服务器从生产者拉，然后消费者再从服务器拉。经常有人提出“前向存储”的生产者。这很有趣，但是我们认为这不符合我们的使用场景，Kafka 的使用场景中会有非常多的生产者。经验告诉我们设计上万个磁盘的系统并不会使得数据更可靠，反而会是噩梦。在实践中，我们发现就算没有生产者持久化，也可以实现大规模运行的强 SLA 数据管道。

### 已消费的位置

跟踪哪些消息被消费了是消息系统的关键性能点之一。

大多数的消息系统把这一信息以元数据的方式存放到服务器上。也就是说，当服务器吧消息发往消费者后，要立即更新本地记录或者等待消费者的 ACK。这看起来是一个相当直观的选择，不过服务器是没办法知道目前消息是否被成功消费的。	因为很多消息系统用于存储的数据结构扩展性不高，所以都倾向于删除已消费的消息。

但是让服务器与消费者对“消息已消费”达成一致并不是一个好解决的问题。如果服务器在把消息发送到网络之后就立即标记消息已消费，那么当消费者没有正确处理消息的时候（比如消费者crash，或者请求超时等等），就会造成消息丢失。为了解决这个问题，许多消息系统都加入了 ACK 机制，即服务器发送消息后，仅标记为已发送，等消费者 ACK 之后在标记为已消费。这种策略可以解决消息丢失的问题，但是也引入了新的问题：第一，如果消费者在发送 ACK 之前，处理消息失败，那么需要再一次消费消息。第二个问题是性能上的，服务器需要处理每一个消息的多个状态（首先是锁住消息不再发第二次，之后是标记为永久消费，可以删除）。这样就需要解决一些比较棘手的问题，比如如何处理已经发送但是一直没有 ACK 的消息。

对于这个问题，Kafka 采用了一种不同的解决方式。在 Kafka 里，一个主题会被划分成多个有序的分区，每一个分区同一时间只能被消费者组里的一个消费者消费。这就意味着只需要一个整数就可以在分区上标记消费者的位置，这个偏移量就是消费者应该消费的下一条消息。这样用来标记消息已消费的数据就只是一个整数。而且这个状态可以被定期检查，比上述 ACK 的方式实现起来更简单。

这样的实现还有一个好处，消费者可以回退偏移量来重新消费消息。这是一个违反队列含义的处理，但对于很多消费场景却很实用。比如消费逻辑有 bug，而且是在消费了一批消息后才被发现，那么就可以通过这种方式重新消费。

### 离线数据加载

Kafka 这种大规模的持久化消息的方式，使得消费者可以实现周期性的批量消费数据，比如周期性的把数据批量加载到 Hadoop 或数仓的脱机系统中。

如果使用 Hadoop，我们可以通过一个个小的 Map 任务，并行的加载数据，每个 节点/主题/分区 对应一个任务。而且 Hadoop 提供任务管理，配合 Kafka 的偏移量可以随时重启那些失败的任务，而不用担心消费做重复的工作。

### 静态成员关系

静态成员关系旨在提高像流式服务、消费者组以及其他构建在重平衡协议之上的应用的可用性。重平衡协议依赖组协调者为组内成员分配 ID，这些生成出来的 ID 会在成员重启然后再加入进来的时候发生改变。对于消费者型的服务，这种“动态的成员关系”会导致大量的 ID 再分配，比如在代码部署、配置更新以及周期性的重启。对于大型的有状态服务，被重排的任务需要时间来恢复它的本地状态，然后才能提供服务。因为观察到这一现象，Kafka 的消费族管理机制允许组成员提供持久的 ID，基于这些 ID，组成员对于 Kafka 的身份可以保持不变，从而不用触发重平衡。

如果你想要使用静态成员关系机制，

* 把 Kafka 集群和客户端的版本提高到 2.3 及以上，并且集群的 `inter.broker.protocol.version` 也要在 2.3版本及以上。
* 把组内每一个消费者的 `ConsumerConfig#GROUP_INSTANCE_ID_CONFIG` 设置成唯一值。
* 对于 Kafka Streams 服务，把 Kafka Streams 实例的 `ConsumerConfig#GROUP_INSTANCE_ID_CONFIG` 设置成唯一值就可以，而不用考虑每个实例里有几个线程。

如果你的服务器版本低于 2.3，那么把客户端的 `ConsumerConfig#GROUP_INSTANCE_ID_CONFIG`，那么客户端会检查服务器版本，如果不满足，则返回 `UnsupportedException`。如果你不小心在不同的实例上设置了相同的 ID，Kafka 集群的保护机制会返回给重复的实例 `org.apache.kafka.common.errors.FencedInstanceIdException`，来通知其关闭。更多详情见[KIP-345](https://cwiki.apache.org/confluence/display/KAFKA/KIP-345%3A+Introduce+static+membership+protocol+to+reduce+consumer+rebalances)。

## 消息传输语义

现在我们稍微了解了一些生产者与消费者的工作流程，让我们来讨论一下 Kafka 提供给生产者消费者之间的语义保证。当然，我们可以提供多种消息传输保证：

* 最多一次——消息可能会丢失，但是不会重复发送。
* 最少一次——消息不会丢失，但是可能会重复发送。
* 有且只有一次——人们当然希望每一条消息会且只会传送一次。

这个问题可以被分解成两个：生产和消费消息这两个通道上的保证。

许多系统都声称自己可以提供“有且只有一次”的语义，但是这些声明中大都具有误导性（比如他们不会涉及生产者或消费者失败的情况，或者有多个消费者消费的情况，或者写入磁盘的数据会丢失的情况）。

Kafka 所提供的语义很直接，当发布消息时，我们有一个消息是否被提交到日志的概念。已经提交过的消息只要携带该分区备份的任何一个服务器还存活的话就不会丢失。下一节中会详细描述已提交消息的定义、活跃分区以及我们可以处理的故障类型的描述。现在，我们先假设有一个不会丢失消息的服务器，通过这个来尝试理解生产者和消费者之间消息传递的保证。当生产者尝试发布一个消息但是遇到一次网络错误时，生产者无法确定错误是发生在提交前还是提交后。这与用自动生成的健插入数据表的语义是一样的（译者注：无法确定自增 ID的具体数值）。

在 0.11.0.0 版本前，生产者如果没有收到提交消息的返回，那么就只能重新发送一次。这就提供了“最少一次”的传输语义，因为消息可能会被重复写入日志中。从 0.11.0.0 版本开始，生产者支持了幂等提交的选项来保证重复发送并不会重复写入日志。为了实现这一功能，服务器需要向生产者分配一个 ID，生产者会给每个消息一个具体的序列号，这样重复的消息就可以被标示出来了。同样是从 0.11.0.0 版本，生产者还支持了类似事务的向多个主题发送消息的语义：要么所有消息均被写入，要么都失败。这一功能的主要用途就是支持“有且仅有一次”的语义。

不是所有的使用场景都需要这么强的保证。在某些对延迟敏感的场景，我们允许生产者制定所需的持久型等级。如果生产者选择等待确认消息是否提交，那么可能需要10 毫秒。生产者也可以选择完全的异步发送，或者只等待分区 Leader 持有此消息。

接下来我们从消费者的角度来考虑。所有的副本中相同消息的偏移量是一样的。基于上面所讲的消息偏移量，消费者可以控制自己在日志中的消费位置。如果消费者永远不会崩溃，那么可以直接把这个位置信息放在内存中，但是当消费者崩溃时，我们希望有一个新的消费进程从合适的位置开始消费。消费者在消费的过程中，对于位置的更新有几种选择：

1. 读消息，然后把位置写入自己的日志中，然后处理消息。在这种情况下，消费者可能在记录位置后，还没处理完消息，就崩溃了。当新的进程来接管消费时，就会漏掉部分消息。这种场景对应着“只多一次”的语义。
2. 读消息，处理消息，然后吸入自己的日志中。在这种情况下，消费者可能会在处理完消息还没更新位置时崩溃。当新的进程接管消费时，就会重复消费部分消息，这种场景对应着“至少一次”的语义。

那么如何实现“有且仅有一次”呢？当从一个主题消费，然后发往另一个主题时（Kafka Streams），我们可以依靠生产者在 0.11.0.0 版本支持的事务功能，消费者消费到的位置是被写入一个特殊的主题的，我们可以将往另一个主题里发处理过的消息和更新位置合并成一个事务。如果这个事务被打断了，那么已处理过的消息和位置都不会存在。

当牵扯到外部系统时，上述方式就有很大的限制了。一个比较经典的方式是使用两阶段提交。不过还有一种比较简单和通用的做法，就是把消息的偏移量和消息处理后的结果写在一起。这种方式的好处在于很多系统不支持两阶段提交。举个例子，[Kafka Connect](https://kafka.apache.org/documentation/#connect) 连接器就会把消息和消息的偏移量一并写入 HDFS，这样两者要么一起更新，要么都不更新。在其他很多系统中，我们也遵从类似的模式。

综上所述，Kafka 在 Kafka Streams 中支持“有且仅有一次”，而在 Kafka 主题之间传输数据时，可以通过事务来实现“有且仅有一次”。而在别的系统中，则需要具体系统的配合，不过 Kafka 通过消息偏移量来使得“有且仅有一次”成为可能。另外，默认情况下，Kafka 保证“至少一次”的传输语义，而且允许用户通过在生产者端关闭重试，在消费者端关闭提交偏移量来实现“只多一次”传输。

## 副本

Kafka 通过配置分区因子在服务器之间复制分区日志（我们可以在主题级别的粒度上设置分区因子）。通过这种方式，我们可以实现自动故障转移，当服务器故障时，仍然有分区副本可用。

其他消息系统也有提供类似副本的功能。但是在我们看来（纯属偏见），这似乎是一个固定的东西，并没有被大量使用，而且有个很大的缺点就是，副本处于非活跃状态，出现问题时吞吐量会收到严重影响，而且需要精巧的配置。Kafka 默认情况下就是使用副本，实际上我们实现的非副本模式就是副本数为1。

一个主题的一个分区是副本的最小单位。在没有故障的情况下，每个分区都有一个领导者和零个或多个跟随者。所有的分区数是包括主分区的。所有的读和写都是走主分区的。通常情况下，总的分区数是比服务器的数目多的，分区需要在服务器之间均匀分配。副本分区和主分区的日志是相同的，所有的消息都是相同的偏移量和顺序（当然，在任何时间，主分区都可能存在还没有被复制的消息）。

跟随者像一个正常的 Kafka 消费者一样从主分区中消费消息，然后再应用到他们自己的日志中。






























