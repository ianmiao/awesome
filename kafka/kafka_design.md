# [译: Kafka Design](https://kafka.apache.org/documentation/#design)

## 动机

Kafka 被设计为一个统一的平台用于处理所有类型的实时数据流，所以我们需要考虑非常多的使用场景。

它需要有较大的吞吐量来支撑像实时日志聚合这样大规模的事件流。

它需要处理庞大的数据积压来支持离线系统定期加载数据。

它同时也需要像传统的消息系统一样支持低延迟的数据发送。

我们希望提供分区的、分布式的、实时的信息流处理方式，这也是我们打造 Kafka 分区和消费模型的灵感源泉。

Kafka 还需要支持将数据流喂给其他数据系统的情况，因此需要有机器故障时的容错能力。

为了支撑上述的使用场景，我们设计了许多独立的元件，与传统的消息系统相比，更像是数据库日志系统。我们将在接下来的几节中概述其中一些元件的设计。

## 持久化

### 不要畏惧文件系统！

Kafka 非常依赖文件系统来存储和缓存消息。人们通常会有一种磁盘很慢的感觉，这使得他们通常会怀疑持久性存储的性能。事实上，磁盘可以很慢也可以很快，这完全取决于如何使用磁盘；一个设计得当的磁盘系统可以网络一样快。

一个非常关键的事实是，在过去的十年里，磁盘驱动的吞吐量和磁盘的寻道延时不匹配。这就导致在一个拥有六个7200rpm SATA RAID-5 阵列的 JBOD 上线性写的性能是 600 MB/秒，而随机写只有 100 KB/秒，相差6000 倍。线性读写在许多使用场景中是可预测的，而且操作系统对此有很多的优化。现代操作系统提供的 Read ahead 和 Write behind 技术可以对数据做大块的预先读取以及把分散的小的逻辑写合并成一个大的物理写入。关于这个问题的讨论，详见[ACM论文](https://queue.acm.org/detail.cfm?id=1563874)，他们甚至发现[在某些情况下，顺序磁盘读取比随机内存访问还快](https://deliveryimages.acm.org/10.1145/1570000/1563874/jacobs3.jpg)

为了弥补磁盘驱动与磁盘寻道性能上的差异，现代操作系统越来越倾向于使用内存做磁盘的缓存。在进行内存回收时，现代系统倾向于使用所有可用内存做磁盘缓存。所有的磁盘读写都统一使用这一缓存系统。除非使用 Direct I/O，否则即使程序已经维护了数据在进程内的高速缓存，操作系统也会在页缓存中再存一遍这些数据。

而且，我们的系统建立在 JVM 上，稍微了解过 Java 内存使用方式的人都应该知道：

1. 对象存储的开销非常高，通常是实际数据大小的一倍（甚至更严重）。
2. 随着对堆内数据的增加，Java 的垃圾回收会越来越慢。

基于上述原因，使用文件系统和页缓存要优于维护内存 cache 和其他结构，至少我们节省出了一倍的缓存，更进一步，使用紧凑的字节结构而非单个对象的存储，可以将所使用的存储空间再节省一半。通过上述方式，我们可以在 32 GB 的计算机上缓存 28-30 GB 的数据，而且没有 GC 的困扰。而且当进程重启时，缓存仍然可用，而如果使用进程内缓存的话，需要在内存中重建数据缓存（10 GB 的缓存可能需要10分钟），或者选择冷启动（初始性能会非常差）。这样做还有一个好处就是代码简洁，由操作系统来处理文件系统和缓存之间的逻辑。如果你的磁盘支持线性读取，那么每次读磁盘时执行的预读会在缓存中填充邻近的有效数据。

这样的设计非常简单，而且我们反对在进程内维护尽可能多的内存，然后在没有可用空间的时候再同步到文件系统的操作。所有的数据直接写入到文件系统而不用刷新到磁盘，实际上只是被放到了内核的页缓存里。

这种以页缓存为中心的设计风格在[这篇文章](http://varnish-cache.org/docs/trunk/phk/notes.html)中进行了描述。

### 常数时间就够了

通常，消息系统里的持久化数据结构都是一个消费者一个队列以及与其关联的 BTree 或其他通用的随机访问数据结构，用于维护消息的元数据。BTree 是最通用的数据结构，可以在消息系统中实现各种事务与非事务语义。但通常也伴随着大量的性能损耗，操作 BTree 时间复杂度是 O(logN)。通常 O(logN) 被认为与常数复杂度相差无几，这在磁盘操作中却不是这样。磁盘的一次寻道是要 10 毫秒，而且每个磁盘同一时间只能执行一次寻道，而且即使是少量的磁盘寻道也会导致很高的性能开销。由于存储系统将非常快的缓存操作和非常慢的磁盘操作混合在一起，当数据不断增加时，树结构的性能损耗通常是超过线性增长的。

直观上讲，一个持久性的队列可以通过对文件的简单读取和追加操作实现，通常和日志记录的解决方案是一样的。这个结构有一个好处就是所有的操作都是O(1)的，而且读操作并不会阻塞写以及其他操作。这样的结构具有明显的性能优势，性能与数据量大小完全脱钩，服务器可以充分利用廉价的1TB 磁盘。尽管这些磁盘的寻道性能较差，但是在大规模数据的读写上性能是可接受的，且价格低廉。

可以访问几乎无限的磁盘空间而不会降低性能，这意味着我们可以提供传统消息系统不具备的功能。例如，Kafka 可以保留已消费的消息很长一段时间，而不用像传统消息系统那样直接删除，这就为消费者提供了非常棒的灵活性。

## 效率

为提高 Kafka 的效率，我们付出了很多努力。一个主要的场景就是处理活跃的网络数据，这个数据量非常大：每个页都可能会产生数十次的写入操作。而且我们假设每一条消息都可能被多个消费者消费，因此我们需要让消费操作尽可能 cheap。

我们从构造和运行很多相似系统的经验发现，这是提高这种多租户操作效率的关键。如果下游基础服务很容易因为业务量的小幅度增加而成为瓶颈，那么这会是一个非常常见的问题。Kafka 可以帮助业务系统在有负载的情况下先于基础服务升级。当在运行支持大集群上数十或数百个业务系统中心化服务时，这是很重要的能力，因为这种使用模式上的变化几乎每天都在发生。

我们在上一节中讨论了磁盘的效率。一旦消除了不良的磁盘访问模式，系统效率低下的常见原因一般有两个：过多的小的I/O操作和过多的字节拷贝。

小 I/O 问题在客户端和服务器之间以及服务器自己的持久化操作中都会发生。

为了避免这个问题，我们的通信协议围绕消息集合这一抽象构建。这样请求就可以将消息组合在一起来分摊网络开销，而不用一次次的发单个的消息。然后服务器再将消息集合一次性添加到持久化日志中，消费者也可以一次性获取大块的消息集合。

这一简单的优化可以把速度提高一个数量级。批量操作带来的大的网络报文，大的连续的磁盘操作，连续的内存快等等，所有这些都可以让 Kafka 将突发的随机消息写入流转换成线性写入流，然后流向消费者。

另外一个低效的问题就是字节拷贝，在消息量较小的时候，这还不算是一个问题，但是当负载比较大时，影响就愈发明显了。为了避免这种情况，我们采用了标准的二进制消息格式，这个格式在生产者、服务器和消费者之间共享（因此消息集合可以在它们之间无修改的传递）。

服务器使用一个目录的文件维护消息日志，每个目录里的文件都由消息集合填充，这些消息集合以生产者消费者可直接使用的格式写入磁盘。保持这种通用的格式可以优化一个非常重要的操作：持久化日志块的网络传输。现代UNIX 操作系统提供了一种非常好的方式可以直接把页缓存发送给套接字。比如 Linux 的 [sendfile](https://man7.org/linux/man-pages/man2/sendfile.2.html)系统调用。

要理解 sendfile 的能力，需要先理解通常数据从文件到 socket 的流转路径：

1. 操作系统将数据从磁盘读到内核的页缓存
2. 应用程序将数据从内核读到用户态的缓存中
3. 应用程序将数据写回到内核中的 socket 缓存中
4. 操作系统将 socket 缓存中的数据拷贝到网卡缓存中，然后发出去

这需要四次拷贝，两次系统调用，显然是效率不高的。使用 sendfile 可以直接把页缓存的数据拷贝到网卡缓存，

我们设想最常见的一种情况，某个主题上有多个消费者，这样消息只需要读取到页缓存中，然后每个消费者的读取都只需要拷贝到网卡，而不需要进入用户态内存空间。这样就可以以接近网络连接的速率消费消息了。

这种页缓存和零拷贝的组合意味着，在一个 Kafka 集群上，可能看不到磁盘读取，完全从缓存中读取数据。

想了解更多 Java 对 sendfile 和零拷贝的支持，可以参考这篇[文章](https://developer.ibm.com/articles/j-zerocopy/)

### 端到端的批量压缩

在某些场景中，瓶颈不在CPU 和磁盘而在网络带宽。对于在广域网里的数据中心之间发送消息的数据管道而言，尤为如此。当然，用户可以直接对消息进行压缩，而无需 Kafka 的支持，但是这样的压缩率不高，因为大量的冗余发生在同类型消息之间的重复（比如 JSON 中的字段名、网络日志中的 user-agents、常见的字符串）。有效的压缩需要把多个消息一起压缩，而不是压缩单个消息。

Kafka 以有效的批处理格式支持批量压缩。可以将以批消息压缩后发给服务器，这批消息将以压缩的形式写入日志，仅由消费者解压缩。

Kafka 支持 GZIP、Snappy 和 ZStandard 压缩协议，有关压缩的更多信息参见[此处](https://cwiki.apache.org/confluence/display/KAFKA/Compression)

## 生产者

### 负载均衡

生产者直接向分区 Leader 发送消息，而不需要经过中间路由层。为达到这一目的，所有的 Kafka 节点都需要知道哪些服务器存活、某个 topic 特定分区的 Leader 是谁。

由客户端控制消息发往哪个分区。这个过程可以是随机的，也可以是负载均衡的，也可以是基于某种规则的。我们暴露了一个分区规则的接口，通过让用户对消息指定一个 Key 并通过对 Key 哈希来进行分区。比如使用 user id 做 Key，那么这个用户的所有数据都会被发送到同一个分区。这样可以让消费者进消费某个特定分区，来达到只接收特定消息的目的。

### 异步发送

批处理是提高效率的主要方式之一，而要启用批处理，Kafka 生产者就需要在内存中累积消息，然后再单个请求中一起发送。批处理可以配置为累计不超过多少条消息，并且等待不超过特定时间。这样可以累积更多字节再发送，让服务器只执行频率较低的大 I/O 操作。这个缓冲是可配置的，并且提供机制来允许少量额外的延迟以提高吞吐量。

关于这个[配置](https://kafka.apache.org/documentation/#producerconfigs)的详情以及生产者的[API](http://kafka.apache.org/082/javadoc/index.html?org/apache/kafka/clients/producer/KafkaProducer.html)可以在别处获取。

## 消费者

Kafka 消费者直接向所要消费的 topic 的特定分区的 Leader 发送 fetch 请求。消费者在请求中携带偏移量，然后就会接收到服务器发来的以对应偏移量开头的日志块。消费者可以通过这个偏移量来重新消费消息。

### 推与拉

我们首先要考虑的第一个问题就是，消费者从服务器拉数据还是服务器直接向消费者推数据。在这一问题上，和大多数的消息系统一样，Kafka 使用了更加传统的设计：生产者推消息到服务器，消费者从服务器拉消息。某些以日志记录为中心的系统，遵循相反的逻辑，直接将数据推送给下游。这两种方式都各自有优缺点。但是基于推送的系统有一个难点是要与各种各样的消费者打交道，服务器要控制推送数据的速率。通常的目标是让消费者以最大的速度消费，但是，在推送系统中，当消费者的速率低于生产者的速率时，消费者往往不知所措（类似一种拒绝服务攻击）。而在拉消息模型中，消费者的速度完全可以低于生产者，并且在可能的情况下跟上。不过在推模型中，也可以使用某种退避协议来表明需要讲推送的速度降下来，但是在这种情况下，充分发挥消费者的消费能力会比较困难。之前基于推模型搭建系统的经验让我们选择了更传统的拉模型。

拉模型的另一个优点是可以更积极的对发送给消费者的消息做批处理。推模型必须要考虑是直接发送当前消息，还是在不知道下游消费者是否有足够消费能力累积消息延迟发送。如果考虑低延时，那么每一条消息都需要单独发送，这样就会造成带宽的浪费。而在拉模型下，消费者总是拉取足够的消息，从日志当前的位置到某个可配置的最大长度。这样就可以在不引入不必要的延迟下，获得最佳的批处理效果。

原始的拉模型有一个不足之处：当服务器没数据时，消费者会陷入一个频繁的轮询中，知道新数据到达。为了避免频发发请求的情况，我们提供了一些配置，可以让消费者阻塞在一个长时间的请求中，直到新消息或者给定字节数的新消息集合到达。

你可以设想别的端到端拉模型的设计。生产者把消息日志写到本地，服务器从生产者拉，然后消费者再从服务器拉。经常有人提出“前向存储”的生产者。这很有趣，但是我们认为这不符合我们的使用场景，Kafka 的使用场景中会有非常多的生产者。经验告诉我们设计上万个磁盘的系统并不会使得数据更可靠，反而会是噩梦。在实践中，我们发现就算没有生产者持久化，也可以实现大规模运行的强 SLA 数据管道。


















